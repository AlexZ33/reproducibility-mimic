{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import psycopg2\n",
    "import sys\n",
    "import datetime as dt\n",
    "import mp_utils as mp\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# used to print out pretty pandas dataframes\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# used to impute mean for data and standardize for computational stability\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# logistic regression is our favourite model ever\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV # l2 regularized regression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# used to calculate AUROC/accuracy\n",
    "from sklearn import metrics\n",
    "\n",
    "# gradient boosting - must download package https://github.com/dmlc/xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "#import matplotlib\n",
    "#import matplotlib.pyplot as plt\n",
    "#from matplotlib.font_manager import FontProperties # for unicode fonts\n",
    "#%matplotlib inline\n",
    "\n",
    "# below config used on pc70\n",
    "sqluser = 'alistairewj'\n",
    "dbname = 'mimic'\n",
    "schema_name = 'mimiciii'\n",
    "query_schema = 'SET search_path to public,' + schema_name + ';'\n",
    "\n",
    "\n",
    "# two options for loading data\n",
    "# option 1) use SQL - requires database and to have run queries/make_all.sql\n",
    "# option 2) use CSVs downloaded\n",
    "USE_SQL=1\n",
    "USE_CSV=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if USE_SQL:\n",
    "    # Connect to local postgres version of mimic\n",
    "    con = psycopg2.connect(dbname=dbname, user=sqluser)\n",
    "\n",
    "    # exclusion criteria:\n",
    "    #   - less than 15 years old\n",
    "    #   - stayed in the ICU less than 4 hours\n",
    "    #   - never have any chartevents data (i.e. likely administrative error)\n",
    "    #   - organ donor accounts (administrative \"readmissions\" for patients who died in hospital)\n",
    "    query = query_schema + \\\n",
    "    \"\"\"\n",
    "    select \n",
    "        *\n",
    "    from dm_cohort\n",
    "    \"\"\"\n",
    "    co = pd.read_sql_query(query,con)\n",
    "    \n",
    "    # convert the inclusion flags to boolean\n",
    "    for c in co.columns:\n",
    "        if c[0:10]=='inclusion_':\n",
    "            co[c] = co[c].astype(bool)\n",
    "\n",
    "    # extract static vars into a separate dataframe\n",
    "    df_static = pd.read_sql_query(query_schema + 'select * from mp_static_data', con)\n",
    "    #for dtvar in ['intime','outtime','deathtime']:\n",
    "    #    df_static[dtvar] = pd.to_datetime(df_static[dtvar])\n",
    "\n",
    "    vars_static = [u'is_male', u'emergency_admission', u'age',\n",
    "                   # services\n",
    "                   u'service_any_noncard_surg',\n",
    "                   u'service_any_card_surg',\n",
    "                   u'service_cmed',\n",
    "                   u'service_traum',\n",
    "                   u'service_nmed',\n",
    "                   # ethnicities\n",
    "                   u'race_black',u'race_hispanic',u'race_asian',u'race_other',\n",
    "                   # phatness\n",
    "                   u'height', u'weight', u'bmi']\n",
    "\n",
    "\n",
    "    # get ~5 million rows containing data from errbody\n",
    "    # this takes a little bit of time to load into memory (~2 minutes)\n",
    "\n",
    "    # %%time results\n",
    "    # CPU times: user 42.8 s, sys: 1min 3s, total: 1min 46s\n",
    "    # Wall time: 2min 7s\n",
    "\n",
    "    df = pd.read_sql_query(query_schema + 'select * from mp_data', con)\n",
    "    df.drop('subject_id',axis=1,inplace=True)\n",
    "    df.drop('hadm_id',axis=1,inplace=True)\n",
    "    df.sort_values(['icustay_id','hr'],axis=0,ascending=True,inplace=True)\n",
    "\n",
    "    # get death information\n",
    "    df_death = pd.read_sql_query(query_schema + \"\"\"\n",
    "    select \n",
    "    co.subject_id, co.hadm_id, co.icustay_id\n",
    "    , ceil(extract(epoch from (co.outtime - co.intime))/60.0/60.0) as dischtime_hours\n",
    "    , ceil(extract(epoch from (adm.deathtime - co.intime))/60.0/60.0) as deathtime_hours\n",
    "    , case when adm.deathtime is null then 0 else 1 end as death\n",
    "    from dm_cohort co\n",
    "    inner join admissions adm\n",
    "    on co.hadm_id = adm.hadm_id\n",
    "    where co.excluded = 0\n",
    "    \"\"\", con)\n",
    "    \n",
    "    # get censoring information\n",
    "    df_censor = pd.read_sql_query(query_schema + \"\"\"\n",
    "    select co.icustay_id, min(cs.charttime) as censortime\n",
    "    , ceil(extract(epoch from min(cs.charttime-co.intime) )/60.0/60.0) as censortime_hours\n",
    "    from dm_cohort co \n",
    "    inner join mp_code_status cs\n",
    "    on co.icustay_id = cs.icustay_id\n",
    "    where cmo+dnr+dni+dncpr+cmo_notes>0\n",
    "    and co.excluded = 0\n",
    "    group by co.icustay_id\n",
    "    \"\"\", con)\n",
    "    \n",
    "    # extract static vars into a separate dataframe\n",
    "    df_static = pd.read_sql_query(query_schema + 'select * from mp_static_data', con)\n",
    "    \n",
    "elif USE_CSV:\n",
    "    co = pd.read_csv('df_cohort.csv.gz')\n",
    "    \n",
    "    # convert the inclusion flags to boolean\n",
    "    for c in co.columns:\n",
    "        if c[0:10]=='inclusion_':\n",
    "            co[c] = co[c].astype(bool)\n",
    "    df = pd.read_csv('df_data.csv.gz')\n",
    "    df_static = pd.read_csv('df_static_data.csv.gz')\n",
    "    df_censor = pd.read_csv('df_censor.csv.gz')\n",
    "    df_death = pd.read_csv('df_death.csv.gz')\n",
    "    \n",
    "else:\n",
    "    print('Must use SQL or CSV to load data!')\n",
    "    \n",
    "    \n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base exclusion criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print out the exclusions *SEQUENTIALLY* - i.e. if already excluded, don't re-print\n",
    "print('Cohort - initial size: {} ICU stays'.format(co.shape[0]))\n",
    "\n",
    "idxRem = np.zeros(co.shape[0],dtype=bool)\n",
    "for c in co.columns:\n",
    "    if c[0:len('exclusion_')]=='exclusion_':\n",
    "        N_REM = np.sum( (co[c].values==1) )\n",
    "        print('  {:5g} ({:2.2f}%) - {}'.format(N_REM,N_REM*100.0/co.shape[0], c))\n",
    "        idxRem[co[c].values==1] = True\n",
    "\n",
    "# summarize all exclusions\n",
    "N_REM = np.sum( idxRem )\n",
    "print('  {:5g} ({:2.2f}%) - {}'.format(N_REM,N_REM*100.0/co.shape[0], 'all exclusions'))\n",
    "print('')\n",
    "print('Final cohort size: {} ICU stays ({:2.2f}%).'.format(co.shape[0] - np.sum(idxRem), (1-np.mean(idxRem))*100.0))\n",
    "co = co.loc[~idxRem,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mortality stats\n",
    "\n",
    "### Mortality in base cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mortality stats for base cohort\n",
    "for c in co.columns:\n",
    "    if c[0:len('death_')]=='death_':\n",
    "        N_ALL = co.shape[0]\n",
    "        N = co.set_index('icustay_id').loc[:,c].sum()\n",
    "        print('{:40s}{:5g} of {:5g} died ({:2.2f}%).'.format(c, N, N_ALL, N*100.0/N_ALL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mortality in MIMIC-II patients staying >= 24 hours\n",
    "\n",
    "This is mainly an example of how the `inclFcn` works. It derives from the cohort a boolean index of patients to retain in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inclFcn = lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_stay_ge_24hr'],'icustay_id']\n",
    "\n",
    "# mortality stats for base cohort\n",
    "for c in co.columns:\n",
    "    if c[0:len('death_')]=='death_':\n",
    "        N_ALL = inclFcn(co).shape[0]\n",
    "        N = co.set_index('icustay_id').loc[inclFcn(co),c].sum()\n",
    "        print('{:40s}{:5g} of {:5g} died ({:2.2f}%).'.format(c, N, N_ALL, N*100.0/N_ALL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the same function in a slightly more obscure way - with the benefit of being able to list all inclusions in a list. This just helps readability in the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inclusions = ['inclusion_only_mimicii', 'inclusion_stay_ge_24hr']\n",
    "inclFcn = lambda x: x.loc[x[inclusions].all(axis=1),'icustay_id']\n",
    "\n",
    "# mortality stats for base cohort\n",
    "for c in co.columns:\n",
    "    if c[0:len('death_')]=='death_':\n",
    "        N_ALL = inclFcn(co).shape[0]\n",
    "        N = co.set_index('icustay_id').loc[inclFcn(co),c].sum()\n",
    "        print('{:40s}{:5g} of {:5g} died ({:2.2f}%).'.format(c, N, N_ALL, N*100.0/N_ALL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclusion criteria\n",
    "\n",
    "Each study has its own exclusion criteria (sometimes studies have multiple experiments). We define a dictionary of all exclusions with the dictionary key as the study name. Some studies have multiple experiments, so we append *a*, *b*, or *c*.\n",
    "\n",
    "The dictionary stores a length 2 list. The first element defines the window for data extraction: it contains a dictionary of the windows and the corresponding window sizes. The second element is the exclusion criteria. Both are functions which use `co` or `df` as their input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first we can define the different windows: there aren't that many!\n",
    "df_tmp=co.copy().set_index('icustay_id')\n",
    "\n",
    "# admission+12 hours\n",
    "time_12hr = df_tmp.copy()\n",
    "time_12hr['windowtime'] = 12\n",
    "time_12hr = time_12hr['windowtime'].to_dict()\n",
    "\n",
    "# admission+24 hours\n",
    "time_24hr = df_tmp.copy()\n",
    "time_24hr['windowtime'] = 24\n",
    "time_24hr = time_24hr['windowtime'].to_dict()\n",
    "\n",
    "# admission+48 hours\n",
    "time_48hr = df_tmp.copy()\n",
    "time_48hr['windowtime'] = 48\n",
    "time_48hr = time_48hr['windowtime'].to_dict()\n",
    "\n",
    "# admission+72 hours\n",
    "time_72hr = df_tmp.copy()\n",
    "time_72hr['windowtime'] = 72\n",
    "time_72hr = time_72hr['windowtime'].to_dict()\n",
    "\n",
    "# admission+96 hours\n",
    "time_96hr = df_tmp.copy()\n",
    "time_96hr['windowtime'] = 96\n",
    "time_96hr = time_96hr['windowtime'].to_dict()\n",
    "\n",
    "# entire stay\n",
    "time_all = df_tmp.copy()\n",
    "time_all = time_all['dischtime_hours'].apply(np.ceil).astype(int).to_dict()\n",
    "\n",
    "# 12 hours before the patient died/discharged\n",
    "time_predeath = df_tmp.copy()\n",
    "time_predeath['windowtime'] = time_predeath['dischtime_hours']\n",
    "idx = time_predeath['deathtime_hours']<time_predeath['dischtime_hours']\n",
    "time_predeath.loc[idx,'windowtime'] = time_predeath.loc[idx,'deathtime_hours']\n",
    "# move from discharge/death time to 12 hours beforehand\n",
    "time_predeath['windowtime'] = time_predeath['windowtime']-12\n",
    "time_predeath = time_predeath['windowtime'].apply(np.ceil).astype(int).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example params used to extract patient data\n",
    "# element 1: dictionary specifying end time of window for each patient\n",
    "# element 2: size of window\n",
    "# element 3: extra hours added to make it easier to get data on labs (and allows us to get labs pre-ICU)\n",
    "# e.g. [time_24hr, 8, 24] is\n",
    "#   (1) window ends at admission+24hr\n",
    "#   (2) window is 8 hours long\n",
    "#   (3) lab window is 8+24=32 hours long\n",
    "\n",
    "def inclFcn(x, inclusions):\n",
    "    return x.loc[x[inclusions].all(axis=1),'icustay_id']\n",
    "\n",
    "\n",
    "# this one is used more than once, so we define it here\n",
    "hugExclFcnMIMIC3 = lambda x: x.loc[x['inclusion_over_18']&x['inclusion_hug2009_obs']&x['inclusion_hug2009_not_nsicu_csicu']&x['inclusion_first_admission']&x['inclusion_full_code']&x['inclusion_not_brain_death']&x['inclusion_not_crf'],'icustay_id'].values\n",
    "hugExclFcn = lambda x: np.intersect1d(hugExclFcnMIMIC3(x),x.loc[x['inclusion_only_mimicii'],'icustay_id'].values)\n",
    "\n",
    "\n",
    "# physionet2012 subset - not exact but close\n",
    "def physChallExclFcn(x):\n",
    "    out = x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_stay_ge_48hr']&x['inclusion_has_saps'],'icustay_id'].values\n",
    "    out = np.sort(out)\n",
    "    out = out[0:4000]\n",
    "    return out\n",
    " \n",
    "# caballero2015 is a random subsample - then limits to 18yrs, resulting in 11648\n",
    "def caballeroExclFcn(x):\n",
    "    out = x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18'],'icustay_id'].values\n",
    "    out = np.sort(out)\n",
    "    out = out[0:11648]\n",
    "    return out\n",
    "\n",
    "np.random.seed(546345)\n",
    "W_extra = 24\n",
    "\n",
    "exclusions = OrderedDict([\n",
    "['caballero2015dynamically_a',  [[time_24hr, 24, W_extra], caballeroExclFcn, 'hospital_expire_flag']],\n",
    "['caballero2015dynamically_b',  [[time_48hr, 48, W_extra], caballeroExclFcn, 'hospital_expire_flag']],\n",
    "['caballero2015dynamically_c',  [[time_72hr, 72, W_extra], caballeroExclFcn, 'hospital_expire_flag']],\n",
    "['calvert2016computational',    [[time_predeath, 5, W_extra], lambda x: x.loc[x['inclusion_over_18']&x['inclusion_only_micu']&x['inclusion_calvert2016_obs']&x['inclusion_stay_ge_17hr']&x['inclusion_stay_le_500hr']&x['inclusion_non_alc_icd9'],'icustay_id'].values, 'hospital_expire_flag']],\n",
    "['calvert2016using',            [[time_predeath, 5, W_extra], lambda x: x.loc[x['inclusion_over_18']&x['inclusion_only_micu']&x['inclusion_calvert2016_obs']&x['inclusion_stay_ge_17hr']&x['inclusion_stay_le_500hr'],'icustay_id'].values, 'hospital_expire_flag']],\n",
    "['celi2012database_a',          [[time_72hr, 72, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_aki_icd9'],'icustay_id'].values , 'hospital_expire_flag']],\n",
    "['celi2012database_b',          [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_sah_icd9'],'icustay_id'].values , 'hospital_expire_flag']],\n",
    "['che2016recurrent_a',          [[time_48hr, 48, W_extra], lambda x: x.loc[x['inclusion_over_18'],'icustay_id'].values , 'death_48hr_post_icu_admit']],\n",
    "['che2016recurrent_b',          [[time_48hr, 48, W_extra], physChallExclFcn , 'hospital_expire_flag']],\n",
    "['ding2016mortality',           [[time_48hr, 48, W_extra], physChallExclFcn , 'hospital_expire_flag']],\n",
    "['ghassemi2014unfolding_a',     [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_ge_100_non_stop_words']&x['inclusion_stay_ge_24hr'],'icustay_id'].values, 'hospital_expire_flag']],\n",
    "['ghassemi2014unfolding_b',     [[time_12hr, 12, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_ge_100_non_stop_words']&x['inclusion_stay_ge_12hr'],'icustay_id'].values, 'hospital_expire_flag']],\n",
    "['ghassemi2014unfolding_c',     [[time_12hr, 12, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_ge_100_non_stop_words']&x['inclusion_stay_ge_12hr'],'icustay_id'].values, 'death_30dy_post_hos_disch']],\n",
    "['ghassemi2014unfolding_d',     [[time_12hr, 12, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_ge_100_non_stop_words']&x['inclusion_stay_ge_12hr'],'icustay_id'].values, 'death_1yr_post_hos_disch']],\n",
    "['ghassemi2015multivariate_a',    [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_ge_100_non_stop_words']&x['inclusion_gt_6_notes']&x['inclusion_stay_ge_24hr']&x['inclusion_has_saps'],'icustay_id'].values, 'hospital_expire_flag']],\n",
    "['ghassemi2015multivariate_b',    [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_ge_100_non_stop_words']&x['inclusion_gt_6_notes']&x['inclusion_stay_ge_24hr']&x['inclusion_has_saps'],'icustay_id'].values, 'death_1yr_post_hos_disch']],\n",
    "['grnarova2016neural_a',          [[time_all,  24, W_extra], lambda x: x.loc[x['inclusion_over_18']&x['inclusion_multiple_hadm'],'icustay_id'].values, 'hospital_expire_flag']],\n",
    "['grnarova2016neural_b',          [[time_all,  24, W_extra], lambda x: x.loc[x['inclusion_over_18']&x['inclusion_multiple_hadm'],'icustay_id'].values, 'death_30dy_post_hos_disch']],\n",
    "['grnarova2016neural_c',          [[time_all,  24, W_extra], lambda x: x.loc[x['inclusion_over_18']&x['inclusion_multiple_hadm'],'icustay_id'].values, 'death_1yr_post_hos_disch']],\n",
    "['harutyunyan2017multitask',    [[time_48hr, 48, W_extra], lambda x: x.loc[x['inclusion_over_18']&x['inclusion_multiple_icustay'],'icustay_id'].values, 'hospital_expire_flag']],\n",
    "['hoogendoorn2016prediction',   [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_hug2009_obs']&x['inclusion_stay_ge_24hr'],'icustay_id'].values, 'hospital_expire_flag']],\n",
    "['hug2009icu',                  [[time_24hr, 24, W_extra], hugExclFcn, 'death_30dy_post_icu_disch']],\n",
    "['johnson2012patient',          [[time_48hr, 48, W_extra], physChallExclFcn, 'hospital_expire_flag']],\n",
    "['johnson2014data',             [[time_48hr, 48, W_extra], physChallExclFcn, 'hospital_expire_flag']],\n",
    "['joshi2012prognostic',         [[time_24hr, 24, W_extra], hugExclFcn, 'hospital_expire_flag']],\n",
    "['joshi2016identifiable',       [[time_48hr, 48, W_extra], lambda x: x.loc[x['inclusion_over_18']&x['inclusion_stay_ge_48hr'],'icustay_id'].values, 'hospital_expire_flag']],\n",
    "['lee2015customization_a',        [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_lee2015_service']&x['inclusion_has_saps']&x['inclusion_stay_ge_24hr'],'icustay_id'].values, 'hospital_expire_flag']],\n",
    "['lee2015customization_b',        [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_lee2015_service']&x['inclusion_has_saps']&x['inclusion_stay_ge_24hr'],'icustay_id'].values, 'death_30dy_post_hos_disch']],\n",
    "['lee2015customization_c',        [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_lee2015_service']&x['inclusion_has_saps']&x['inclusion_stay_ge_24hr'],'icustay_id'].values, 'death_2yr_post_hos_disch']],\n",
    "['lee2015personalized',         [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_has_saps']&x['inclusion_stay_ge_24hr'],'icustay_id'].values, 'death_30dy_post_hos_disch']],\n",
    "['lee2017patient',              [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_has_saps']&x['inclusion_stay_ge_24hr'],'icustay_id'].values, 'death_30dy_post_hos_disch']],\n",
    "['lehman2012risk',              [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_has_saps']&x['inclusion_stay_ge_24hr']&x['inclusion_first_admission'],'icustay_id'].values, 'hospital_expire_flag']],\n",
    "['luo2016interpretable_a',        [[time_all,  24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_has_sapsii']&x['inclusion_no_disch_summary'],'icustay_id'].values, 'death_30dy_post_hos_disch']],\n",
    "['luo2016interpretable_b',        [[time_all,  24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_has_sapsii']&x['inclusion_no_disch_summary'],'icustay_id'].values, 'death_6mo_post_hos_disch']],\n",
    "['luo2016predicting',           [[time_24hr, 12, W_extra], lambda x: np.intersect1d(hugExclFcn(x),x.loc[x['inclusion_stay_ge_24hr'],'icustay_id'].values) , 'death_30dy_post_icu_disch']],\n",
    "['pirracchio2015mortality',     [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii'],'icustay_id'].values , 'hospital_expire_flag']],\n",
    "['ripoll2014sepsis',            [[time_24hr, 24, W_extra], lambda x: x.loc[x['inclusion_only_mimicii']&x['inclusion_over_18']&x['inclusion_has_saps']&x['inclusion_not_explicit_sepsis'],'icustay_id'].values, 'hospital_expire_flag']],\n",
    "['wojtusiak2017c',              [[time_all,  24, W_extra], lambda x: x.loc[x['inclusion_over_65']&x['inclusion_alive_hos_disch'],'icustay_id'].values, 'death_30dy_post_hos_disch']]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare sample sizes and mortality rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "repro_stats = pd.DataFrame(None, columns=['N_Repro','Y_Repro'])\n",
    "\n",
    "N = co.shape[0]\n",
    "    \n",
    "for current_study in exclusions:\n",
    "    params, iid_keep, y_outcome_label = exclusions[current_study]\n",
    "    \n",
    "    # iid_keep is currently a function - apply it to co to get ICUSTAY_IDs to keep for this study\n",
    "    iid_keep = iid_keep(co)\n",
    "    \n",
    "    # add outcome to df_data\n",
    "    df_data = df_data.merge(co.set_index('icustay_id')[[y_outcome_label]], how='left',\n",
    "                            left_index=True, right_index=True)\n",
    "    \n",
    "    \n",
    "    N_STUDY = iid_keep.shape[0]\n",
    "    Y_STUDY = co.set_index('icustay_id').loc[iid_keep,y_outcome_label].mean()*100.0\n",
    "    \n",
    "    # print size of cohort in study\n",
    "    print('{:5g} ({:5.2f}%) - Mortality = {:5.2f}% - {}'.format(\n",
    "            N_STUDY, N_STUDY*100.0/N, Y_STUDY,\n",
    "            current_study)\n",
    "         )\n",
    "    \n",
    "    repro_stats.loc[current_study] = [N_STUDY, Y_STUDY]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above dataframe, `repro_stats`, we can compare our results to those extracted manually from the studies. We load in the manual extraction from the `data` subfolder, merge it with this dataframe, and output to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "study_data = pd.read_csv('../data/study_data_output.csv')\n",
    "study_data.drop(['N_Repro','Y_Repro','GB','LR'],axis=1,inplace=True)\n",
    "study_data.to_csv('../data/study_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "study_data = pd.read_csv('../data/study_data.csv')\n",
    "study_data.set_index('Cohort',inplace=True)\n",
    "# add in reproduction sample size // outcome\n",
    "study_data_merged = study_data.merge(repro_stats, how='left',\n",
    "                left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "# print out the table as it was in the paper (maybe a bit more precision)\n",
    "study_data_merged[ ['N_Study','N_Repro','Y_Study','Y_Repro'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define K-folds for AUROC comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define var_static which is used later\n",
    "#TODO: should refactor so this isn't needed\n",
    "var_min, var_max, var_first, var_last, var_sum, var_first_early, var_last_early, var_static = mp.vars_of_interest()\n",
    "\n",
    "K=5\n",
    "np.random.seed(871)\n",
    "# get unique subject_id (this is needed later)\n",
    "sid = np.sort(np.unique(df_death['subject_id'].values))\n",
    "\n",
    "# assign k-fold\n",
    "idxK_sid = np.random.permutation(sid.shape[0])\n",
    "idxK_sid = np.mod(idxK_sid,K)\n",
    "\n",
    "# get indices which map subject_ids in sid to the X dataframe\n",
    "idxMap = np.searchsorted(sid, df_death['subject_id'].values)\n",
    "\n",
    "# use these indices to map the k-fold integers\n",
    "idxK = idxK_sid[idxMap]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run results for a single study\n",
    "\n",
    "The below code cell:\n",
    "\n",
    "* extracts the cohort for a specific study\n",
    "* extracts the outcome of that study\n",
    "* builds predictive models in 5-fold cross-validation for that outcome\n",
    "\n",
    "The two models at the moment are Gradient Boosting (xgboost) and logistic regression (scikit-learn).\n",
    "This code cell only runs for one study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pick the study to run the example on\n",
    "current_study = 'celi2012database_b'\n",
    "    \n",
    "# Rough timing info:\n",
    "#     rf - 3 seconds per fold\n",
    "#    xgb - 30 seconds per fold\n",
    "# logreg - 4 seconds per fold\n",
    "#  lasso - 8 seconds per fold\n",
    "models = OrderedDict([\n",
    "          ['xgb', xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)],\n",
    "          #['lasso', LassoCV(cv=5,fit_intercept=True,normalize=True,max_iter=10000)],\n",
    "          #['rf', RandomForestClassifier()],\n",
    "          ['logreg', LogisticRegression(fit_intercept=True)]\n",
    "         ])\n",
    "\n",
    "print('')\n",
    "print('====================={}==========='.format('='*len(current_study)))\n",
    "print('========== BEGINNING {}==========='.format(current_study))\n",
    "print('====================={}==========='.format('='*len(current_study)))\n",
    "\n",
    "params = exclusions[current_study][0]\n",
    "df_data = mp.get_design_matrix(df, params[0], W=params[1], W_extra=params[2])\n",
    "\n",
    "# get a list of icustay_id who stayed at least 12 hours\n",
    "iid_keep = exclusions[current_study][1](co)\n",
    "print('Reducing sample size from {} to {} ({:2.2f}%).'.format(\n",
    "        df_data.shape[0], iid_keep.shape[0], iid_keep.shape[0]*100.0 / df_data.shape[0]))\n",
    "df_data = df_data.loc[iid_keep,:]\n",
    "print('')\n",
    "\n",
    "y_outcome_label = exclusions[current_study][2]\n",
    "\n",
    "# load the data into a numpy array\n",
    "\n",
    "# first, the data from static vars from df_static\n",
    "X = df_data.merge(df_static.set_index('icustay_id')[var_static], how='left', left_index=True, right_index=True)\n",
    "# next, add in the outcome: death in hospital\n",
    "X = X.merge(co.set_index('icustay_id')[[y_outcome_label]], left_index=True, right_index=True)\n",
    "\n",
    "# map above K-fold indices to this dataset\n",
    "X = X.merge(co.set_index('icustay_id')[['subject_id']], left_index=True, right_index=True)\n",
    "# get indices which map subject_ids in sid to the X dataframe\n",
    "idxMap = np.searchsorted(sid, X['subject_id'].values)\n",
    "# use these indices to map the k-fold integers\n",
    "idxK = idxK_sid[idxMap]\n",
    "# drop the subject_id column\n",
    "X.drop('subject_id',axis=1,inplace=True)\n",
    "\n",
    "# convert to numpy data (assumes target, death, is the last column)\n",
    "X = X.values\n",
    "y = X[:,-1]\n",
    "X = X[:,0:-1]\n",
    "X_header = [x for x in df_data.columns.values] + var_static\n",
    "\n",
    "mdl_val = dict()\n",
    "results_val = dict()\n",
    "pred_val = dict()\n",
    "tar_val = dict()\n",
    "\n",
    "for mdl in models:\n",
    "    print('=============== {} ==============='.format(mdl))\n",
    "    mdl_val[mdl] = list()\n",
    "    results_val[mdl] = list() # initialize list for scores\n",
    "    pred_val[mdl] = list()\n",
    "    tar_val[mdl] = list()\n",
    "\n",
    "    if mdl == 'xgb':\n",
    "        # no pre-processing of data necessary for xgb\n",
    "        estimator = Pipeline([(mdl, models[mdl])])\n",
    "\n",
    "    else:\n",
    "        estimator = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"scaler\", StandardScaler()),\n",
    "                      (mdl, models[mdl])]) \n",
    "\n",
    "    for k in range(K):\n",
    "        # train the model using all but the kth fold\n",
    "        curr_mdl = estimator.fit(X[idxK != k, :],y[idxK != k])\n",
    "\n",
    "        # get prediction on this dataset\n",
    "        if mdl == 'lasso':\n",
    "            curr_prob = curr_mdl.predict(X[idxK == k, :])\n",
    "        else:\n",
    "            curr_prob = curr_mdl.predict_proba(X[idxK == k, :])\n",
    "            curr_prob = curr_prob[:,1]\n",
    "\n",
    "        pred_val[mdl].append(curr_prob)\n",
    "        tar_val[mdl].append(y[idxK == k])\n",
    "\n",
    "        # calculate score (AUROC)\n",
    "        curr_score = metrics.roc_auc_score(y[idxK == k], curr_prob)\n",
    "\n",
    "        # add score to list of scores\n",
    "        results_val[mdl].append(curr_score)\n",
    "\n",
    "        # save the current model\n",
    "        mdl_val[mdl].append(curr_mdl)\n",
    "\n",
    "        print('{} - Finished fold {} of {}. AUROC {:0.3f}.'.format(dt.datetime.now(), k+1, K, curr_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run results for all results\n",
    "\n",
    "The below code block is identical to the above code block, except it loops over all studies evaluated. This code block takes a while - it is training ~150 models of each type. The final AUROCs are output to the `results.txt` file. The final models/results/predictions/targets are saved in various dictionaries with the suffix `_all`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdl_val_all = dict()\n",
    "results_val_all = dict()\n",
    "pred_val_all = dict()\n",
    "tar_val_all = dict()\n",
    "\n",
    "# Rough timing info:\n",
    "#     rf - 3 seconds per fold\n",
    "#    xgb - 30 seconds per fold\n",
    "# logreg - 4 seconds per fold\n",
    "#  lasso - 8 seconds per fold\n",
    "models = OrderedDict([\n",
    "          ['xgb', xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)],\n",
    "          #['lasso', LassoCV(cv=5,fit_intercept=True,normalize=True,max_iter=10000)],\n",
    "          #['rf', RandomForestClassifier()],\n",
    "          ['logreg', LogisticRegression(fit_intercept=True)]\n",
    "         ])\n",
    "    \n",
    "with open('results.txt','w') as fp:\n",
    "    fp.write('StudyName,SampleSize,Outcome')\n",
    "    for mdl in models:\n",
    "        fp.write(',{}'.format(mdl))\n",
    "    fp.write('\\n')\n",
    "    \n",
    "for current_study in exclusions:    \n",
    "    print('\\n==================== {} =========='.format('='*len(current_study)))\n",
    "    print('========== BEGINNING {} =========='.format(current_study))\n",
    "    print('==================== {} =========='.format('='*len(current_study)))\n",
    "\n",
    "    params = exclusions[current_study][0]\n",
    "    df_data = mp.get_design_matrix(df, params[0], W=params[1], W_extra=params[2])\n",
    "\n",
    "    # get a list of icustay_id who stayed at least 12 hours\n",
    "    iid_keep = exclusions[current_study][1](co)\n",
    "    print('Reducing sample size from {} to {} ({:2.2f}%).'.format(\n",
    "            df_data.shape[0], iid_keep.shape[0], iid_keep.shape[0]*100.0 / df_data.shape[0]))\n",
    "    df_data = df_data.loc[iid_keep,:]\n",
    "    print('')\n",
    "\n",
    "    y_outcome_label = exclusions[current_study][2]\n",
    "    \n",
    "    # load the data into a numpy array\n",
    "\n",
    "    # first, the data from static vars from df_static\n",
    "    X = df_data.merge(df_static.set_index('icustay_id')[var_static], how='left', left_index=True, right_index=True)\n",
    "    # next, add in the outcome: death in hospital\n",
    "    X = X.merge(co.set_index('icustay_id')[[y_outcome_label]], left_index=True, right_index=True)\n",
    "\n",
    "    # map above K-fold indices to this dataset\n",
    "    X = X.merge(co.set_index('icustay_id')[['subject_id']], left_index=True, right_index=True)\n",
    "    # get indices which map subject_ids in sid to the X dataframe\n",
    "    idxMap = np.searchsorted(sid, X['subject_id'].values)\n",
    "    # use these indices to map the k-fold integers\n",
    "    idxK = idxK_sid[idxMap]\n",
    "    # drop the subject_id column\n",
    "    X.drop('subject_id',axis=1,inplace=True)\n",
    "\n",
    "    # convert to numpy data (assumes target, death, is the last column)\n",
    "    X = X.values\n",
    "    y = X[:,-1]\n",
    "    X = X[:,0:-1]\n",
    "    X_header = [x for x in df_data.columns.values] + var_static\n",
    "    \n",
    "    mdl_val = dict()\n",
    "    results_val = dict()\n",
    "    pred_val = dict()\n",
    "    tar_val = dict()\n",
    "\n",
    "    for mdl in models:\n",
    "        print('=============== {} ==============='.format(mdl))\n",
    "        mdl_val[mdl] = list()\n",
    "        results_val[mdl] = list() # initialize list for scores\n",
    "        pred_val[mdl] = list()\n",
    "        tar_val[mdl] = list()\n",
    "\n",
    "        if mdl == 'xgb':\n",
    "            # no pre-processing of data necessary for xgb\n",
    "            estimator = Pipeline([(mdl, models[mdl])])\n",
    "\n",
    "        else:\n",
    "            estimator = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                              strategy=\"mean\",\n",
    "                                              axis=0)),\n",
    "                          (\"scaler\", StandardScaler()),\n",
    "                          (mdl, models[mdl])]) \n",
    "\n",
    "        for k in range(K):\n",
    "            # train the model using all but the kth fold\n",
    "            curr_mdl = estimator.fit(X[idxK != k, :],y[idxK != k])\n",
    "\n",
    "            # get prediction on this dataset\n",
    "            if mdl == 'lasso':\n",
    "                curr_prob = curr_mdl.predict(X[idxK == k, :])\n",
    "            else:\n",
    "                curr_prob = curr_mdl.predict_proba(X[idxK == k, :])\n",
    "                curr_prob = curr_prob[:,1]\n",
    "\n",
    "            pred_val[mdl].append(curr_prob)\n",
    "            tar_val[mdl].append(y[idxK == k])\n",
    "\n",
    "            # calculate score (AUROC)\n",
    "            curr_score = metrics.roc_auc_score(y[idxK == k], curr_prob)\n",
    "\n",
    "            # add score to list of scores\n",
    "            results_val[mdl].append(curr_score)\n",
    "\n",
    "            # save the current model\n",
    "            mdl_val[mdl].append(curr_mdl)\n",
    "\n",
    "            print('{} - Finished fold {} of {}. AUROC {:0.3f}.'.format(dt.datetime.now(), k+1, K, curr_score))\n",
    "\n",
    "    # create a pointer for above dicts with new var names\n",
    "    # we will likely re-use the dicts in subsequent calls for getting model perfomances\n",
    "    mdl_val_all[current_study] = mdl_val\n",
    "    results_val_all[current_study] = results_val\n",
    "    pred_val_all[current_study] = pred_val\n",
    "    tar_val_all[current_study] = tar_val\n",
    "    \n",
    "    # print to file\n",
    "    with open('results.txt','a') as fp:\n",
    "        # print study name, sample size and frequency of outcome\n",
    "        fp.write( '{},{},{:2.2f}'.format(current_study, X.shape[0], np.mean(y)*100.0 ) )\n",
    "        \n",
    "        for i, mdl in enumerate(models):\n",
    "            fp.write(',{:0.6f}'.format( np.mean(results_val[mdl]) ))\n",
    "        \n",
    "        fp.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the results from the above dictionaries into a single dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdl_list = models.keys()\n",
    "\n",
    "study_data = pd.read_csv('../data/study_data.csv')\n",
    "study_data.set_index('Cohort',inplace=True)\n",
    "\n",
    "# add in reproduction stats from earlier\n",
    "study_data_merged = study_data.merge(repro_stats, how='left',\n",
    "                left_index=True, right_index=True)\n",
    "\n",
    "# add in AUROCs\n",
    "for current_study in results_val_all:\n",
    "    results_val = results_val_all[current_study]\n",
    "    for mdl in results_val:\n",
    "        study_data_merged.loc[current_study, mdl] = np.mean(results_val[mdl])\n",
    "        \n",
    "study_data_merged[ ['Outcome','N_Study','N_Repro','Y_Study','Y_Repro','AUROC_Study'] + mdl_list ]\n",
    "\n",
    "study_data_merged.to_csv('results_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Baseline model 1\n",
    "\n",
    "The below code block builds a \"baseline\" model. This model has no exclusions past the base cohort exclusions. K-fold validation is done on the patient level to ensure no information leakage between training/validation sets. The outcome is in-hospital mortality, and the data window used is the first 24 hours. Labs are extracted from up to 24 hours before the ICU admission (this is defined by `W_extra=params[2]=24`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Rough timing info:\n",
    "#     rf - 3 seconds per fold\n",
    "#    xgb - 30 seconds per fold\n",
    "# logreg - 4 seconds per fold\n",
    "#  lasso - 8 seconds per fold\n",
    "models = OrderedDict([\n",
    "          ['xgb', xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)],\n",
    "          #['lasso', LassoCV(cv=5,fit_intercept=True,normalize=True,max_iter=10000)],\n",
    "          #['rf', RandomForestClassifier()],\n",
    "          ['logreg', LogisticRegression(fit_intercept=True)]\n",
    "         ])\n",
    "current_study = 'baseline'\n",
    "\n",
    "print('')\n",
    "print('====================={}==========='.format('='*len(current_study)))\n",
    "print('========== BEGINNING {} =========='.format(current_study))\n",
    "print('====================={}==========='.format('='*len(current_study)))\n",
    "\n",
    "\n",
    "# admission+24 hours\n",
    "df_tmp=co.copy().set_index('icustay_id')\n",
    "time_dict = df_tmp.copy()\n",
    "time_dict['windowtime'] = 24\n",
    "time_dict = time_dict['windowtime'].to_dict()\n",
    "\n",
    "params = [time_dict, 24, 24]\n",
    "iid_keep = co['icustay_id'].values # include all icustays\n",
    "y_outcome_label = 'death_in_hospital'\n",
    "\n",
    "df_data = mp.get_design_matrix(df, params[0], W=params[1], W_extra=params[2])\n",
    "\n",
    "# get a list of icustay_id who stayed at least 12 hours\n",
    "N_NEW=df_data.loc[iid_keep,:].shape[0]\n",
    "print('Reducing sample size from {} to {} ({:2.2f}%).'.format(\n",
    "        co.shape[0], N_NEW, N_NEW*100.0 / co.shape[0]))\n",
    "df_data = df_data.loc[iid_keep,:]\n",
    "print('')\n",
    "\n",
    "# load the data into a numpy array\n",
    "\n",
    "# first, the data from static vars from df_static\n",
    "X = df_data.merge(df_static.set_index('icustay_id')[var_static], how='left', left_index=True, right_index=True)\n",
    "# next, add in the outcome: death in hospital\n",
    "X = X.merge(co.set_index('icustay_id')[[y_outcome_label]], left_index=True, right_index=True)\n",
    "\n",
    "# map above K-fold indices to this dataset\n",
    "X = X.merge(co.set_index('icustay_id')[['subject_id']], left_index=True, right_index=True)\n",
    "# get indices which map subject_ids in sid to the X dataframe\n",
    "idxMap = np.searchsorted(sid, X['subject_id'].values)\n",
    "# use these indices to map the k-fold integers\n",
    "idxK = idxK_sid[idxMap]\n",
    "# drop the subject_id column\n",
    "X.drop('subject_id',axis=1,inplace=True)\n",
    "\n",
    "# convert to numpy data (assumes target, death, is the last column)\n",
    "X = X.values\n",
    "y = X[:,-1]\n",
    "X = X[:,0:-1]\n",
    "X_header = [x for x in df_data.columns.values] + var_static\n",
    "\n",
    "mdl_val = dict()\n",
    "results_val = dict()\n",
    "pred_val = dict()\n",
    "tar_val = dict()\n",
    "\n",
    "for mdl in models:\n",
    "    print('=============== {} ==============='.format(mdl))\n",
    "    mdl_val[mdl] = list()\n",
    "    results_val[mdl] = list() # initialize list for scores\n",
    "    pred_val[mdl] = list()\n",
    "    tar_val[mdl] = list()\n",
    "\n",
    "    if mdl == 'xgb':\n",
    "        # no pre-processing of data necessary for xgb\n",
    "        estimator = Pipeline([(mdl, models[mdl])])\n",
    "\n",
    "    else:\n",
    "        estimator = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"scaler\", StandardScaler()),\n",
    "                      (mdl, models[mdl])]) \n",
    "\n",
    "    for k in range(K):\n",
    "        # train the model using all but the kth fold\n",
    "        curr_mdl = estimator.fit(X[idxK != k, :],y[idxK != k])\n",
    "\n",
    "        # get prediction on this dataset\n",
    "        if mdl == 'lasso':\n",
    "            curr_prob = curr_mdl.predict(X[idxK == k, :])\n",
    "        else:\n",
    "            curr_prob = curr_mdl.predict_proba(X[idxK == k, :])\n",
    "            curr_prob = curr_prob[:,1]\n",
    "\n",
    "        pred_val[mdl].append(curr_prob)\n",
    "        tar_val[mdl].append(y[idxK == k])\n",
    "\n",
    "        # calculate score (AUROC)\n",
    "        curr_score = metrics.roc_auc_score(y[idxK == k], curr_prob)\n",
    "\n",
    "        # add score to list of scores\n",
    "        results_val[mdl].append(curr_score)\n",
    "\n",
    "        # save the current model\n",
    "        mdl_val[mdl].append(curr_mdl)\n",
    "\n",
    "        print('{} - Finished fold {} of {}. AUROC {:0.3f}.'.format(dt.datetime.now(), k+1, K, curr_score))\n",
    "        \n",
    "\n",
    "\n",
    "# print final results\n",
    "print('StudyName,SampleSize',end='')\n",
    "for mdl in models:\n",
    "    print(',{}'.format(mdl),end='')\n",
    "print('')\n",
    "\n",
    "print( '{},{}'.format(current_study, X.shape[0] ), end='' )\n",
    "\n",
    "for i, mdl in enumerate(models):\n",
    "    print(',{:0.6f}'.format( np.mean(results_val[mdl]) ), end='')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model 2: No care withdrawal patients\n",
    "\n",
    "Patients who choose to have their care withdrawn will receive palliative measures in the ICU. These patients show markedly different physiology than those undergoing full interventions and a model which synthesizes severity should not incorporate their data. Here we remove data for patients at the time of their withdrawal of care. If this is before the end of the first 24 hours of their ICU admission, we remove the patient entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CENSOR_FLAG = True\n",
    "# Rough timing info:\n",
    "#     rf - 3 seconds per fold\n",
    "#    xgb - 30 seconds per fold\n",
    "# logreg - 4 seconds per fold\n",
    "#  lasso - 8 seconds per fold\n",
    "models = OrderedDict([\n",
    "          ['xgb', xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)],\n",
    "          #['lasso', LassoCV(cv=5,fit_intercept=True,normalize=True,max_iter=10000)],\n",
    "          #['rf', RandomForestClassifier()],\n",
    "          ['logreg', LogisticRegression(fit_intercept=True)]\n",
    "         ])\n",
    "current_study = 'baseline_censored'\n",
    "\n",
    "print('')\n",
    "print('====================={}==========='.format('='*len(current_study)))\n",
    "print('========== BEGINNING {} =========='.format(current_study))\n",
    "print('==================== {}==========='.format('='*len(current_study)))\n",
    "\n",
    "\n",
    "# admission+24 hours\n",
    "df_tmp=co.copy().set_index('icustay_id')\n",
    "time_dict = df_tmp.copy()\n",
    "time_dict['windowtime'] = 24\n",
    "time_dict = time_dict['windowtime'].to_dict()\n",
    "\n",
    "params = [time_dict, 24, 24]\n",
    "# optionally remove patients who were DNR in first 24hrs\n",
    "if CENSOR_FLAG:\n",
    "    idx = (df_tmp['censortime_hours'].isnull()) | (df_tmp['censortime_hours']>=24)\n",
    "    iid_keep = df_tmp.loc[idx, :].index.values\n",
    "else:\n",
    "    iid_keep = df_tmp['icustay_id'].values # include all icustays\n",
    "y_outcome_label = 'death_in_hospital'\n",
    "\n",
    "df_data = mp.get_design_matrix(df, params[0], W=params[1], W_extra=params[2])\n",
    "\n",
    "# get a list of icustay_id who stayed at least 12 hours\n",
    "N_NEW=df_data.loc[iid_keep,:].shape[0]\n",
    "print('Reducing sample size from {} to {} ({:2.2f}%).'.format(\n",
    "        co.shape[0], N_NEW, N_NEW*100.0 / co.shape[0]))\n",
    "df_data = df_data.loc[iid_keep,:]\n",
    "print('')\n",
    "\n",
    "# load the data into a numpy array\n",
    "\n",
    "# first, the data from static vars from df_static\n",
    "X = df_data.merge(df_static.set_index('icustay_id')[var_static], how='left', left_index=True, right_index=True)\n",
    "# next, add in the outcome: death in hospital\n",
    "X = X.merge(co.set_index('icustay_id')[[y_outcome_label]], left_index=True, right_index=True)\n",
    "\n",
    "# map above K-fold indices to this dataset\n",
    "X = X.merge(co.set_index('icustay_id')[['subject_id']], left_index=True, right_index=True)\n",
    "# get indices which map subject_ids in sid to the X dataframe\n",
    "idxMap = np.searchsorted(sid, X['subject_id'].values)\n",
    "# use these indices to map the k-fold integers\n",
    "idxK = idxK_sid[idxMap]\n",
    "# drop the subject_id column\n",
    "X.drop('subject_id',axis=1,inplace=True)\n",
    "\n",
    "# convert to numpy data (assumes target, death, is the last column)\n",
    "X = X.values\n",
    "y = X[:,-1]\n",
    "X = X[:,0:-1]\n",
    "X_header = [x for x in df_data.columns.values] + var_static\n",
    "\n",
    "mdl_val = dict()\n",
    "results_val = dict()\n",
    "pred_val = dict()\n",
    "tar_val = dict()\n",
    "\n",
    "for mdl in models:\n",
    "    print('=============== {} ==============='.format(mdl))\n",
    "    mdl_val[mdl] = list()\n",
    "    results_val[mdl] = list() # initialize list for scores\n",
    "    pred_val[mdl] = list()\n",
    "    tar_val[mdl] = list()\n",
    "\n",
    "    if mdl == 'xgb':\n",
    "        # no pre-processing of data necessary for xgb\n",
    "        estimator = Pipeline([(mdl, models[mdl])])\n",
    "\n",
    "    else:\n",
    "        estimator = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"scaler\", StandardScaler()),\n",
    "                      (mdl, models[mdl])]) \n",
    "\n",
    "    for k in range(K):\n",
    "        # train the model using all but the kth fold\n",
    "        curr_mdl = estimator.fit(X[idxK != k, :],y[idxK != k])\n",
    "\n",
    "        # get prediction on this dataset\n",
    "        if mdl == 'lasso':\n",
    "            curr_prob = curr_mdl.predict(X[idxK == k, :])\n",
    "        else:\n",
    "            curr_prob = curr_mdl.predict_proba(X[idxK == k, :])\n",
    "            curr_prob = curr_prob[:,1]\n",
    "\n",
    "        pred_val[mdl].append(curr_prob)\n",
    "        tar_val[mdl].append(y[idxK == k])\n",
    "\n",
    "        # calculate score (AUROC)\n",
    "        curr_score = metrics.roc_auc_score(y[idxK == k], curr_prob)\n",
    "\n",
    "        # add score to list of scores\n",
    "        results_val[mdl].append(curr_score)\n",
    "\n",
    "        # save the current model\n",
    "        mdl_val[mdl].append(curr_mdl)\n",
    "\n",
    "        print('{} - Finished fold {} of {}. AUROC {:0.3f}.'.format(dt.datetime.now(), k+1, K, curr_score))\n",
    "        \n",
    "\n",
    "\n",
    "# print final results\n",
    "print('StudyName,SampleSize',end='')\n",
    "for mdl in models:\n",
    "    print(',{}'.format(mdl),end='')\n",
    "print('')\n",
    "\n",
    "print( '{},{}'.format(current_study, X.shape[0] ), end='' )\n",
    "\n",
    "for i, mdl in enumerate(models):\n",
    "    print(',{:0.6f}'.format( np.mean(results_val[mdl]) ), end='')\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CENSOR_FLAG = True\n",
    "# Rough timing info:\n",
    "#     rf - 3 seconds per fold\n",
    "#    xgb - 30 seconds per fold\n",
    "# logreg - 4 seconds per fold\n",
    "#  lasso - 8 seconds per fold\n",
    "models = OrderedDict([\n",
    "          ['xgb', xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)],\n",
    "          #['lasso', LassoCV(cv=5,fit_intercept=True,normalize=True,max_iter=10000)],\n",
    "          ['logreg', LogisticRegression(fit_intercept=True)]\n",
    "          #['rf', RandomForestClassifier()]\n",
    "         ])\n",
    "current_study = 'baseline_withdrawal'\n",
    "\n",
    "print('====================={}==========='.format('='*len(current_study)))\n",
    "print('========== BEGINNING {} =========='.format(current_study))\n",
    "print('====================={}==========='.format('='*len(current_study)))\n",
    "\n",
    "\n",
    "# admission+24 hours\n",
    "df_tmp=co.copy().set_index('icustay_id')\n",
    "time_dict = df_tmp.copy()\n",
    "time_dict['windowtime'] = 24\n",
    "time_dict = time_dict['windowtime'].to_dict()\n",
    "\n",
    "# optionally remove patients who were DNR in first 24hrs\n",
    "if CENSOR_FLAG:\n",
    "    exclFcn = lambda x: x.loc[x['inclusion_stay_ge_24hr']& ( (x['censortime_hours'].isnull()) | (x['censortime_hours']>=24) ) ,'icustay_id'].values\n",
    "else:\n",
    "    exclFcn = lambda x: x.loc[x['inclusion_stay_ge_24hr']& ( (x['censortime_hours'].isnull()) | (x['censortime_hours']>=24) ) ,'icustay_id'].values\n",
    "    \n",
    "y_outcome_label = 'death_in_hospital'\n",
    "df_data = mp.get_design_matrix(df, params[0], W=params[1], W_extra=params[2])\n",
    "\n",
    "iid_keep = exclFcn(co)\n",
    "N_NEW=df_data.loc[iid_keep,:].shape[0]\n",
    "\n",
    "print('Reducing sample size from {} to {} ({:2.2f}%).'.format(\n",
    "        co.shape[0], N_NEW, N_NEW*100.0 / df_data.shape[0]))\n",
    "df_data = df_data.loc[iid_keep,:]\n",
    "print('')\n",
    "\n",
    "# load the data into a numpy array\n",
    "\n",
    "# first, the data from static vars from df_static\n",
    "X = df_data.merge(df_static.set_index('icustay_id')[var_static], how='left', left_index=True, right_index=True)\n",
    "# next, add in the outcome: death in hospital\n",
    "X = X.merge(co.set_index('icustay_id')[[y_outcome_label]], left_index=True, right_index=True)\n",
    "\n",
    "# map above K-fold indices to this dataset\n",
    "X = X.merge(co.set_index('icustay_id')[['subject_id']], left_index=True, right_index=True)\n",
    "# get indices which map subject_ids in sid to the X dataframe\n",
    "idxMap = np.searchsorted(sid, X['subject_id'].values)\n",
    "# use these indices to map the k-fold integers\n",
    "idxK = idxK_sid[idxMap]\n",
    "# drop the subject_id column\n",
    "X.drop('subject_id',axis=1,inplace=True)\n",
    "\n",
    "# convert to numpy data (assumes target, death, is the last column)\n",
    "X = X.values\n",
    "y = X[:,-1]\n",
    "X = X[:,0:-1]\n",
    "X_header = [x for x in df_data.columns.values] + var_static\n",
    "\n",
    "mdl_val = dict()\n",
    "results_val = dict()\n",
    "pred_val = dict()\n",
    "tar_val = dict()\n",
    "\n",
    "for mdl in models:\n",
    "    print('=============== {} ==============='.format(mdl))\n",
    "    mdl_val[mdl] = list()\n",
    "    results_val[mdl] = list() # initialize list for scores\n",
    "    pred_val[mdl] = list()\n",
    "    tar_val[mdl] = list()\n",
    "\n",
    "    if mdl == 'xgb':\n",
    "        # no pre-processing of data necessary for xgb\n",
    "        estimator = Pipeline([(mdl, models[mdl])])\n",
    "\n",
    "    else:\n",
    "        estimator = Pipeline([(\"imputer\", Imputer(missing_values='NaN',\n",
    "                                          strategy=\"mean\",\n",
    "                                          axis=0)),\n",
    "                      (\"scaler\", StandardScaler()),\n",
    "                      (mdl, models[mdl])]) \n",
    "\n",
    "    for k in range(K):\n",
    "        # train the model using all but the kth fold\n",
    "        curr_mdl = estimator.fit(X[idxK != k, :],y[idxK != k])\n",
    "\n",
    "        # get prediction on this dataset\n",
    "        if mdl == 'lasso':\n",
    "            curr_prob = curr_mdl.predict(X[idxK == k, :])\n",
    "        else:\n",
    "            curr_prob = curr_mdl.predict_proba(X[idxK == k, :])\n",
    "            curr_prob = curr_prob[:,1]\n",
    "\n",
    "        pred_val[mdl].append(curr_prob)\n",
    "        tar_val[mdl].append(y[idxK == k])\n",
    "\n",
    "        # calculate score (AUROC)\n",
    "        curr_score = metrics.roc_auc_score(y[idxK == k], curr_prob)\n",
    "\n",
    "        # add score to list of scores\n",
    "        results_val[mdl].append(curr_score)\n",
    "\n",
    "        # save the current model\n",
    "        mdl_val[mdl].append(curr_mdl)\n",
    "\n",
    "        print('{} - Finished fold {} of {}. AUROC {:0.3f}.'.format(dt.datetime.now(), k+1, K, curr_score))\n",
    "        \n",
    "\n",
    "\n",
    "# print final results\n",
    "print('StudyName,SampleSize',end='')\n",
    "for mdl in models:\n",
    "    print(',{}'.format(mdl),end='')\n",
    "print('')\n",
    "\n",
    "print( '{},{}'.format(current_study, X.shape[0] ), end='' )\n",
    "\n",
    "for i, mdl in enumerate(models):\n",
    "    print(',{:0.6f}'.format( np.mean(results_val[mdl]) ), end='')\n",
    "\n",
    "print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
